{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T21:50:51.837691Z",
     "start_time": "2021-04-20T21:50:46.904509Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "# NLP Libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "#import scispacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy import displacy\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# ONE-TIME DOWNLOADS\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "#### Below is seen identical shape and datatypes in each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T21:50:53.023861Z",
     "start_time": "2021-04-20T21:50:53.003138Z"
    }
   },
   "outputs": [],
   "source": [
    "syn2019 = pd.read_csv('../Data/GOUT-CC-2019-CORPUS-SYNTHETIC.csv')\n",
    "syn2020 = pd.read_csv('../Data/GOUT-CC-2020-CORPUS-SYNTHETIC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both files are identical, containing\n",
    "-  column 0: string text of the patient's chief complaint\n",
    "-  column 1: string indicator indicating the prediction made by the ER Physician on the possibility of Gout\n",
    "-  column 2: string indicator indicating the consensus result by the endocrinologist on the diagnosis of Gout\n",
    "\n",
    "string indicator is a char of Y (yes), N (no), or - (unknown) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Chief Complaint Predict Consensus\n",
      "0  \"been feeling bad\" last 2 weeks & switched BP ...       N         -\n",
      "1  \"can't walk\", reports onset at 0830 am. orient...       Y         N\n",
      "2  \"dehydration\" Chest hurts, hips hurt, cramps P...       Y         Y\n",
      "3  \"gout flare up\" L arm swelling x 1 week. denie...       Y         Y\n",
      "4  \"heart racing,\"dyspnea, and orthopnea that has...       N         -\n"
     ]
    }
   ],
   "source": [
    "print(syn2019.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Chief Complaint Predict Consensus\n",
      "0  \"I dont know whats going on with my head, its ...       N         -\n",
      "1  \"i've been depressed for a few weeks now, i'm ...       N         -\n",
      "2  Altercation while making arrest, c/o R hand pa...       N         N\n",
      "3  Cut on L upper thigh wtih saw. Bleeding contro...       N         N\n",
      "4   Dysuria x1 week. hx: hysterectomy, gerd, bipolar       N         -\n"
     ]
    }
   ],
   "source": [
    "print(syn2020.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the files into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8437, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([syn2019, syn2020], axis=0).reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Complaint    object\n",
      "Predict            object\n",
      "Consensus          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the target values and remove nulls, in this case \"U\" is considered a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    8168\n",
       "U     156\n",
       "Y     111\n",
       "-       2\n",
       "Name: Predict, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Predict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-    7976\n",
       "N     350\n",
       "Y      95\n",
       "U      16\n",
       "Name: Consensus, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T21:50:53.453142Z",
     "start_time": "2021-04-20T21:50:53.445734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Consensus']!= \"-\"]\n",
    "df = df[df['Predict']!= \"-\"]\n",
    "df = df[df['Consensus']!= \"U\"]\n",
    "df = df[df['Predict']!= \"U\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Complaint    object\n",
      "Predict            object\n",
      "Consensus          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    209\n",
       "Y     94\n",
       "Name: Predict, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Predict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    223\n",
       "Y     80\n",
       "Name: Consensus, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Consensus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove digits: \n",
    "#### the text commonly contains many descriptions such as number of times a patient takes medication, etc and will add noise to the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Chief Complaint Predict Consensus\n",
      "1   \"can't walk\", reports onset at  am. oriented x...       Y         N\n",
      "2   \"dehydration\" Chest hurts, hips hurt, cramps P...       Y         Y\n",
      "3   \"gout flare up\" L arm swelling x  week. denies...       Y         Y\n",
      "5   \"I started breathing hard\"  hx- htn, gout, anx...       N         N\n",
      "6   \"I think I have a gout flare up\" L wrist pain ...       Y         Y\n",
      "7   \"I want to see if I have an infection\" pt vagu...       Y         N\n",
      "8   \"My gout done flared up on me\", c/o R ankle, L...       Y         Y\n",
      "9   \"my gout is hurting me\"- reports bilateral foo...       Y         Y\n",
      "11        \"umbearable\" right foot/ankle pain pmh gout       Y         Y\n",
      "12  (LMC) transfer for renal transplant. pt SOB x ...       N         N\n",
      "14  (JI) dizziness and SOB. recent HTN med change ...       N         N\n",
      "15  (RM) intermittent cp since this am, worse duri...       N         N\n",
      "17  (Grandview tx) neck pain and right arm pain fo...       N         N\n",
      "18  (Stat call) epigastric pain and hypotension si...       N         N\n",
      "19  abd pain, NV x  year, worse over last  months....       N         N\n",
      "21  Aching in chest, sides, shoulders, pain with i...       N         N\n",
      "23  AMS, lethargy, increasing generalized weakness...       N         N\n",
      "27  bil. knee pain for \"a while\". fell earlier tod...       Y         N\n",
      "28  bilat foot poain, r hand pain r/t gout per pat...       Y         N\n",
      "31  bilateral knee swelling Left sided chest pain,...       Y         N\n"
     ]
    }
   ],
   "source": [
    "df['Chief Complaint'] = df['Chief Complaint'].apply(lambda x: re.sub(pattern, '', x))\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T21:50:54.279789Z",
     "start_time": "2021-04-20T21:50:54.277277Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Chief Complaint'] = df['Chief Complaint'].apply(lambda x: tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:40:56.920982Z",
     "start_time": "2021-04-19T18:40:56.917975Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:41:40.404946Z",
     "start_time": "2021-04-19T18:41:39.381062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chief Complaint</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[can't, walk, reports, onset, oriented, x, aor...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[dehydration, chest, hurts, hips, hurt, cramps...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[gout, flare, l, arm, swelling, x, week, denie...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[started, breathing, hard, hx, htn, gout, anxi...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[think, gout, flare, l, wrist, pain, swelling,...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>[sob, right, chest, pain, x, weeks, hx, multip...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>[starts, lower, back, goes, right, legs, x, mo...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8427</th>\n",
       "      <td>[sciatica, pain, r, lower, back, radiating, r,...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>[stepped, nail, home, right, foot, painful, di...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>[rash, sores, across, body, infection, ro, lef...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Chief Complaint Predict Consensus\n",
       "1     [can't, walk, reports, onset, oriented, x, aor...       Y         N\n",
       "2     [dehydration, chest, hurts, hips, hurt, cramps...       Y         Y\n",
       "3     [gout, flare, l, arm, swelling, x, week, denie...       Y         Y\n",
       "5     [started, breathing, hard, hx, htn, gout, anxi...       N         N\n",
       "6     [think, gout, flare, l, wrist, pain, swelling,...       Y         Y\n",
       "...                                                 ...     ...       ...\n",
       "8424  [sob, right, chest, pain, x, weeks, hx, multip...       N         N\n",
       "8425  [starts, lower, back, goes, right, legs, x, mo...       N         N\n",
       "8427  [sciatica, pain, r, lower, back, radiating, r,...       N         N\n",
       "8432  [stepped, nail, home, right, foot, painful, di...       N         N\n",
       "8436  [rash, sores, across, body, infection, ro, lef...       N         N\n",
       "\n",
       "[303 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Chief Complaint'] = df['Chief Complaint'].apply(lambda x :remove_stopwords(x))\n",
    "df['Chief Complaint'].head()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:42:32.337243Z",
     "start_time": "2021-04-19T18:42:32.334960Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:43:37.868494Z",
     "start_time": "2021-04-19T18:43:37.865957Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:43:38.537198Z",
     "start_time": "2021-04-19T18:43:38.504486Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Chief Complaint'] = df['Chief Complaint'].apply(lambda x: word_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:44:05.326830Z",
     "start_time": "2021-04-19T18:44:05.324300Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:44:38.554220Z",
     "start_time": "2021-04-19T18:44:38.551293Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_stemmer(text):\n",
    "    stem_text = ' '.join([stemmer.stem(i) for i in text])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:45:01.646162Z",
     "start_time": "2021-04-19T18:45:01.504680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chief Complaint</th>\n",
       "      <th>Predict</th>\n",
       "      <th>Consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can't walk report onset orient x aortic valv r...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dehydr chest hurt hip hurt cramp pmh hip repla...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gout flare l arm swell x week deni pmh</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>start breath hard hx htn gout anxieti</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>think gout flare l wrist pain swell sinc hx af...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8424</th>\n",
       "      <td>sob right chest pain x week hx multipl back su...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>start lower back go right leg x month chiropr ...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8427</th>\n",
       "      <td>sciatica pain r lower back radiat r groin x wk...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>step nail home right foot pain difficult walk ...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>rash sore across bodi infect ro left thumb sta...</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Chief Complaint Predict Consensus\n",
       "1     can't walk report onset orient x aortic valv r...       Y         N\n",
       "2     dehydr chest hurt hip hurt cramp pmh hip repla...       Y         Y\n",
       "3                gout flare l arm swell x week deni pmh       Y         Y\n",
       "5                 start breath hard hx htn gout anxieti       N         N\n",
       "6     think gout flare l wrist pain swell sinc hx af...       Y         Y\n",
       "...                                                 ...     ...       ...\n",
       "8424  sob right chest pain x week hx multipl back su...       N         N\n",
       "8425  start lower back go right leg x month chiropr ...       N         N\n",
       "8427  sciatica pain r lower back radiat r groin x wk...       N         N\n",
       "8432  step nail home right foot pain difficult walk ...       N         N\n",
       "8436  rash sore across bodi infect ro left thumb sta...       N         N\n",
       "\n",
       "[303 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Chief Complaint'] = df['Chief Complaint'].apply(lambda x: word_stemmer(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Nouns contained in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gout', 226),\n",
       " ('pain', 216),\n",
       " ('htn', 134),\n",
       " ('pmh', 120),\n",
       " ('x', 99),\n",
       " ('pt', 90),\n",
       " ('day', 73),\n",
       " ('c', 68),\n",
       " ('dm', 58),\n",
       " ('deni', 56)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into sentences && flatten nested lists\n",
    "sentences = [nltk.pos_tag(s.split()) for s in df['Chief Complaint']]\n",
    "flat_s = [item for sublist in sentences for item in sublist]\n",
    "\n",
    "# get nouns from tagged words\n",
    "nouns = [(word, tag) for word, tag in flat_s if any(noun_tag in tag for noun_tag in ['NP', 'NN'])]\n",
    "\n",
    "# build frequency distribution for nouns\n",
    "nouns_freq = nltk.FreqDist([word for word, tag in nouns])\n",
    "\n",
    "# view top 10 occuring nouns\n",
    "nouns_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:55:34.783914Z",
     "start_time": "2021-04-19T18:55:34.781731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N    209\n",
      "Y     94\n",
      "Name: Predict, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "predict = df.Predict\n",
    "print(predict.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:55:50.896916Z",
     "start_time": "2021-04-19T18:55:50.894317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N    223\n",
      "Y     80\n",
      "Name: Consensus, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "concensus = df.Consensus\n",
    "print(concensus.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:57:46.020618Z",
     "start_time": "2021-04-19T18:57:46.016646Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, X_test, y_train, y_test = train_test_split(df['Chief Complaint'], predict, test_size=0.33, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:58:16.463073Z",
     "start_time": "2021-04-19T18:58:16.460612Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:58:38.590991Z",
     "start_time": "2021-04-19T18:58:38.571840Z"
    }
   },
   "outputs": [],
   "source": [
    "count_train = count_vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:58:55.137020Z",
     "start_time": "2021-04-19T18:58:55.131924Z"
    }
   },
   "outputs": [],
   "source": [
    "count_test = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T18:59:09.477210Z",
     "start_time": "2021-04-19T18:59:09.473604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abd', 'abdomen', 'abdomin', 'abl', 'abnorm', 'abus', 'abx', 'accept', 'accid', 'ach']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T19:03:22.817035Z",
     "start_time": "2021-04-19T19:03:22.796577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abd', 'abdomen', 'abdomin', 'abl', 'abnorm', 'abus', 'abx', 'accept', 'accid', 'ach']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model: testing 'alpha' hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.78\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.87\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.79\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.79\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.77\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.75\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.76\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.74\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.74\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mimicnlp2/lib/python3.8/site-packages/sklearn/naive_bayes.py:511: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Nultinomial Naive Bayes Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T19:05:03.100247Z",
     "start_time": "2021-04-19T19:05:03.086931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=0.1)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix: 0=Gout, 1 = non-Gout\n",
    "#### Below is seen 3 false negatives, 10 False Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots confusion matrix of model\n",
    "\n",
    "def plot_confusion(y_test, pred):\n",
    "    mat = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "    plt.xlabel('true')\n",
    "    plt.ylabel('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion(y_test, pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.86      0.95      0.91        66\n",
      "           Y       0.89      0.71      0.79        34\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.88      0.83      0.85       100\n",
      "weighted avg       0.87      0.87      0.87       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier coefficients\n",
    "#### Interesting to see the terms associated with Gout and terms indicating non-gout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words indicating Gout:  ['gout' 'pain' 'knee' 'foot' 'swell' 'hx' 'htn' 'day' 'week' 'pmh']\n",
      "Least likely to indicate Gout:  ['pain' 'pmh' 'htn' 'gout' 'pt' 'day' 'deni' 'right' 'chest' 'dm']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted = nb_classifier.feature_log_prob_[0, :].argsort()[::-1]\n",
    "pos_class_prob_sorted = nb_classifier.feature_log_prob_[1, :].argsort()[::-1]\n",
    "\n",
    "print(\"Top 10 words indicating Gout: \",np.take(count_vectorizer.get_feature_names(), pos_class_prob_sorted[:10]))\n",
    "print(\"Least likely to indicate Gout: \",np.take(count_vectorizer.get_feature_names(), neg_class_prob_sorted[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
